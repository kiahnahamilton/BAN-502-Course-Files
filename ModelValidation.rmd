---
output:
  word_document: default
  html_document: default
---
# Module 3 - Assignment 1, Model Validation

## Kiahna Hamilton

### BAN 502 - Dr. Hill 
```{r}
library(tidyverse)
library(MASS)
library(caret)

bike <- read_csv("hour (1).csv")

bike = bike%>%mutate(season = as_factor(as.character(season))) %>%
  mutate(season = fct_recode(season,
                             "Spring" = "1",
                             "Summer" = "2",
                             "Fall" = "3",
                             "Winter" = "4"))
  bike = bike %>%mutate(yr = as_factor(as.character(yr)))
  bike = bike %>% mutate(mnth = as_factor(as.character(mnth)))
  bike = bike %>% mutate(hr = as_factor(as.character(hr)))
  bike = bike %>% mutate(holiday = as_factor(as.character(holiday)))
  bike = bike %>% mutate(holiday =as_factor(as.character(holiday)))%>% mutate(holiday =fct_recode(holiday,"NotHoliday" = "0","Holiday" = "1")) 
bike = bike %>% mutate(workingday =as_factor(as.character(workingday)))%>% mutate(workingday =fct_recode(workingday,"NotWorkingDay" = "0","WorkingDay" = "1"))
bike = bike %>% mutate(weekday =as_factor(as.character(weekday)))%>% mutate(weekday =fct_recode(weekday,"Sunday" = "0","Monday" = "1","Tuesday" = "2","Wednesday" = "3","Thursday" = "4","Friday" = "5","Saturday" = "6"))
bike = bike %>% mutate(weathersit =as_factor(as.character(weathersit)))%>% mutate(weathersit =fct_recode(weathersit,"NoPrecip" = "1","Misty" = "2","LightPrecip" = "3","HeavyPrecip" = "4")) 
glimpse(bike)                                                               
```

***Task 2***
```{r}
set.seed(1234)
train.rows = createDataPartition(y =bike$count,p=0.7,list = FALSE)
train = bike[train.rows,]
test = bike[-train.rows,]
```
  In the training set, there are 12,167 rows of data. In the testing set, there are 5,212 rows of data. 
  
***Task 3***
```{r}
mod1 = lm(count ~ season + mnth + hr + holiday + weekday + temp + weathersit, train)
summary(mod1)
```
  From our regression, we see that we end with an adjusted r-squared value of .6202. Along with this, many of our variables show to have significant values. Based on these findings, we can assume that this model may be good to follow in making predictions. 
  
  
***Task 4***
```{r}
predict_train <- predict(mod1, newdata = train)
head(predict_train)
```
The training set is not the best predictor of the data as all the values but one are negative. Since we are predicting the counts of rides per hour in the bikeshare program, it would not be possible for the service to have negative amounts of rides. This would not be the best model for the data. 

***Task 5*
```{r}
predict_test <- predict(mod1, newdata = test)
head(predict_test)
```
  The test set is a better set to make predictions with the data based off our model as the values are positive (minus two). However, the range in these values gives me reason to believe that our model may be flawed as the values are not all within the same range. But, this may be a more realistic indication of the amount of rides per hour within the bike share service. 
***Task 6***
```{r}
SSE = sum((test$count - predict_test)^2)
SST = sum((test$count - mean(test$count))^2)
1 - SSE/SST
```
  This value is fairly close to the r-squared value from the model made with the training set. Within the model, our value was .6202. Here we have a value of .6289. Since these values are so close, we can say that this model should be a good predictor of values. 
  
***Task 7***
```{r}
ctrl = trainControl(method="cv",number=10)
set.seed(123)
modCV=train(count ~ season + mnth + hr + holiday + weekday + temp + weathersit, train, method = "lm",trControl=ctrl,metric="Rsquared")
summary(modCV)
```
  The k-fold cross-validation differs from model validation in that there is an equal chance for all observations to appear in both training and test sets. From doing this, we see that our adjusted r-squared value is .6206, and multiple r-squared is .6217, which matches our original value from our first regression model. This being said, we should say that this model is a reliable model to use when making predictions due to the fact that these values are consistent with one another. 